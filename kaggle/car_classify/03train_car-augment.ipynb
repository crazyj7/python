{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train car - v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data ; crop and resize  \n",
    "ImageAugment  \n",
    "network ; mobilenetv2, Dense 2  \n",
    "\n",
    "train acc ; 99, val acc; 79,  max lb acc ; 80    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from keras.applications import mobilenetv2\n",
    "from keras.preprocessing import image\n",
    "# import cv2\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dropout, MaxPooling2D, Flatten, Dense, Softmax\n",
    "import random\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.models import Input, Model\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath='car.h5'\n",
    "cache=False\n",
    "basedir = './'\n",
    "\n",
    "x_trainall = np.load('x_train.npy')\n",
    "y_trainall = np.load('y_train.npy')\n",
    "dfclass = pd.read_csv(basedir+'class.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgwidth=224\n",
    "imgheight=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 224, 224, 3) (7000,) (2990, 224, 224, 3) (2990,)\n",
      "0 195\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "cvmode=1\n",
    "datacnt = x_trainall.shape[0]\n",
    "if cvmode==0:\n",
    "    # 7000, 2990\n",
    "    print('cvmode=0')\n",
    "    x_train = x_trainall[:7000]\n",
    "    y_train = y_trainall[:7000]\n",
    "    x_val = x_trainall[7000:]\n",
    "    y_val = y_trainall[7000:]\n",
    "elif cvmode==1:\n",
    "    print('cvmode=1')\n",
    "    x_train = x_trainall[datacnt-7000:]\n",
    "    y_train = y_trainall[datacnt-7000:]\n",
    "    x_val = x_trainall[:datacnt-7000]\n",
    "    y_val = y_trainall[:datacnt-7000]\n",
    "    \n",
    "del x_trainall\n",
    "del y_trainall\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\n",
    "print(np.min(y_train), np.max(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train_onehot = np_utils.to_categorical(y_train, 196)\n",
    "y_val_onehot = np_utils.to_categorical(y_val, 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Augumentation\n",
    "batch_size=64\n",
    "datagen1 = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest')\n",
    "datagen2 = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = datagen1.flow(x_train, y_train_onehot, batch_size=batch_size)\n",
    "val_generator = datagen2.flow(x_val, y_val_onehot, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint save weights in progress...\n",
    "if not os.path.exists('ckpt'):\n",
    "    os.mkdir('ckpt')\n",
    "checkpoint_path='ckpt/check_{epoch:04d}.ckpt'\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, period=10)\n",
    "\n",
    "# tensorboard log\n",
    "if not os.path.exists('log'):\n",
    "    os.mkdir('log')\n",
    "tensorboard = TensorBoard(log_dir='log/'+str(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache model use... continuous!\n"
     ]
    }
   ],
   "source": [
    "force = False\n",
    "\n",
    "if os.path.exists(modelpath) and force==False :\n",
    "    cache=True\n",
    "    print('cache model use... continuous!')\n",
    "else:\n",
    "    cache=False\n",
    "    print('no cache model. create new model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_pad (ZeroPadding2D)       (None, 225, 225, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 112, 112, 32) 864         Conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_3 (GlobalM (None, 1280)         0           out_relu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 224)          286944      global_max_pooling2d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 196)          44100       dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "softmax_3 (Softmax)             (None, 196)          0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,589,028\n",
      "Trainable params: 331,044\n",
      "Non-trainable params: 2,257,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "if cache==True:\n",
    "    model = load_model(modelpath)\n",
    "    print('load model...')\n",
    "else:\n",
    "    inputs = Input(shape=(224,224,3))\n",
    "    net = mobilenetv2.MobileNetV2(input_tensor=inputs, input_shape=(224, 224, 3), alpha=1.0, include_top=False, weights='imagenet', pooling='max')\n",
    "    net2 = Dense(224, activation='relu') (net.layers[-1].output)\n",
    "    net2 = Dense(196)(net2)\n",
    "    net2 = Softmax(196)(net2)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=net2)\n",
    "    print('new model...')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7f7b536370b8> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f7b53637128> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53637208> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53637240> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53637390> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b53637470> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53637588> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b536375f8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b536378d0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53637908> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53637a90> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53637ba8> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53637d30> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f7b53637e48> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b53637e80> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5363f048> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b5363f240> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b5363f278> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5363f400> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b5363f518> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5363f6a0> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b5363f7b8> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b5363f7f0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5363f860> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b5363fb38> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b5363fb70> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5363fcf8> False\n",
      "<keras.layers.merge.Add object at 0x7f7b5363fe10> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b5363fe48> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53637eb8> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53647128> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f7b53647160> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b53647198> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53647208> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b536474e0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53647518> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b536476a0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b536477b8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53647940> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53647a58> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b53647a90> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53647b00> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53647dd8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53647e10> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53647f98> False\n",
      "<keras.layers.merge.Add object at 0x7f7b5363ffd0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b5364e128> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5364e2b0> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b5364e3c8> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b5364e400> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5364e470> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b5364e748> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b5364e780> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5364e908> False\n",
      "<keras.layers.merge.Add object at 0x7f7b5364ea20> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b5364ea58> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5364ebe0> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b5364ecf8> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f7b5364ed30> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b5364ed68> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5364edd8> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53647fd0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53655128> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b536552b0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b536553c8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53655550> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53655668> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b536556a0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53655710> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b536559e8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53655a20> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53655ba8> False\n",
      "<keras.layers.merge.Add object at 0x7f7b53655cc0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53655cf8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53655e80> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53655f98> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b5364efd0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5365d080> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b5365d358> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b5365d390> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5365d518> False\n",
      "<keras.layers.merge.Add object at 0x7f7b5365d630> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b5365d668> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5365d7f0> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b5365d908> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b5365d940> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5365d9b0> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b5365dc88> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b5365dcc0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b5365de48> False\n",
      "<keras.layers.merge.Add object at 0x7f7b5365df60> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b5365df98> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53663160> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53663278> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b536632b0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53663320> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b536635f8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53663630> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b536637b8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b536638d0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53663a58> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53663b70> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b53663ba8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53663c18> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53663ef0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53663f28> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b536680f0> False\n",
      "<keras.layers.merge.Add object at 0x7f7b53668208> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53668240> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b536683c8> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b536684e0> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b53668518> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53668588> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53668860> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53668898> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53668a20> False\n",
      "<keras.layers.merge.Add object at 0x7f7b53668b38> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53668b70> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53668cf8> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53668e10> False\n",
      "<keras.layers.convolutional.ZeroPadding2D object at 0x7f7b53668e48> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b53668e80> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53655fd0> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53670208> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53670240> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b536703c8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b536704e0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53670668> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53670780> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b536707b8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53670828> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53670b00> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53670b38> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53670cc0> False\n",
      "<keras.layers.merge.Add object at 0x7f7b53670dd8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53670e10> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b53670f98> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b53668ef0> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b535f7128> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b535f7198> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b535f7470> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b535f74a8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b535f7630> False\n",
      "<keras.layers.merge.Add object at 0x7f7b535f7748> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b535f7780> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b535f7908> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b535f7a20> False\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x7f7b535f7a58> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b535f7ac8> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b535f7da0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b535f7dd8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b535f7f60> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f7b53670fd0> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f7b535fd240> False\n",
      "<keras.layers.advanced_activations.ReLU object at 0x7f7b535fd358> True\n",
      "<keras.layers.pooling.GlobalMaxPooling2D object at 0x7f7b535fd390> True\n",
      "<keras.layers.core.Dense object at 0x7f7b535fd400> True\n",
      "<keras.layers.core.Dense object at 0x7f7b535fd550> True\n",
      "<keras.layers.advanced_activations.Softmax object at 0x7f7b535fd6a0> True\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_pad (ZeroPadding2D)       (None, 225, 225, 3)  0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 112, 112, 32) 864         Conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 112, 112, 32) 128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 112, 112, 32) 0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 112, 112, 32) 288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 112, 112, 32) 128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 112, 112, 32) 0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 112, 112, 16) 512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 112, 112, 16) 64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 112, 112, 96) 1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 112, 112, 96) 384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 112, 112, 96) 0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 113, 113, 96) 0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 56, 56, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 56, 56, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 56, 56, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 56, 56, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 56, 56, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 56, 56, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 56, 56, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 56, 56, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 56, 56, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 56, 56, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 56, 56, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 56, 56, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 56, 56, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 56, 56, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 56, 56, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 57, 57, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 28, 28, 144)  1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 28, 28, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 28, 28, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 28, 28, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 28, 28, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 28, 28, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 28, 28, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 28, 28, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 28, 28, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 28, 28, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 28, 28, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 28, 28, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 28, 28, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 28, 28, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 28, 28, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 28, 28, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 28, 28, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 28, 28, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 28, 28, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 29, 29, 192)  0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 14, 14, 192)  1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 14, 14, 192)  768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 14, 14, 192)  0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 14, 14, 64)   12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 14, 14, 64)   256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 14, 14, 384)  24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 14, 14, 64)   24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 14, 14, 64)   256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 14, 14, 64)   0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 14, 14, 384)  24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 14, 14, 64)   24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 14, 14, 64)   256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 14, 14, 64)   0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 14, 14, 384)  24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 14, 14, 384)  1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 14, 14, 384)  0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 14, 14, 384)  3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 14, 14, 384)  1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 14, 14, 384)  0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 14, 14, 64)   24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 14, 14, 64)   256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 14, 14, 64)   0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 14, 14, 384)  24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 14, 14, 384)  1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 14, 14, 384)  0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 14, 14, 384)  3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 14, 14, 384)  1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 14, 14, 96)   36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 14, 14, 96)   384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 14, 14, 576)  55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 14, 14, 96)   55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 14, 14, 96)   384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 14, 14, 96)   0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 14, 14, 576)  55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 14, 14, 576)  5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 14, 14, 576)  2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 14, 14, 96)   55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 14, 14, 96)   384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 14, 14, 96)   0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 14, 14, 576)  55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 14, 14, 576)  2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 14, 14, 576)  0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 15, 15, 576)  0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 7, 7, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 7, 7, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 7, 7, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 7, 7, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 7, 7, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 7, 7, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 7, 7, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 7, 7, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 7, 7, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 7, 7, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 7, 7, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 7, 7, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 7, 7, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 7, 7, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 7, 7, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 7, 7, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 7, 7, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 7, 7, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 7, 7, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 7, 7, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 7, 7, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_3 (GlobalM (None, 1280)         0           out_relu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 224)          286944      global_max_pooling2d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 196)          44100       dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "softmax_3 (Softmax)             (None, 196)          0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,589,028\n",
      "Trainable params: 331,044\n",
      "Non-trainable params: 2,257,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##### model freeze\n",
    "if cache==True:\n",
    "    for layer in model.layers[:-5]:\n",
    "        layer.trainable=False\n",
    "    for layer in model.layers:\n",
    "        print(layer, layer.trainable)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    model.load_weights('ckpt/check_0180.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "110/109 [==============================] - 50s 450ms/step - loss: 0.0316 - acc: 0.9943 - val_loss: 0.1415 - val_acc: 0.9669\n",
      "Epoch 2/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0523 - acc: 0.9885 - val_loss: 0.1571 - val_acc: 0.9656\n",
      "Epoch 3/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0424 - acc: 0.9887 - val_loss: 0.1394 - val_acc: 0.9679\n",
      "Epoch 4/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0479 - acc: 0.9895 - val_loss: 0.1421 - val_acc: 0.9689\n",
      "Epoch 5/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0447 - acc: 0.9913 - val_loss: 0.1515 - val_acc: 0.9672\n",
      "Epoch 6/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0491 - acc: 0.9884 - val_loss: 0.1801 - val_acc: 0.9602\n",
      "Epoch 7/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0464 - acc: 0.9911 - val_loss: 0.1780 - val_acc: 0.9602\n",
      "Epoch 8/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0467 - acc: 0.9884 - val_loss: 0.1739 - val_acc: 0.9619\n",
      "Epoch 9/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0450 - acc: 0.9917 - val_loss: 0.1572 - val_acc: 0.9689\n",
      "Epoch 10/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0373 - acc: 0.9912 - val_loss: 0.1435 - val_acc: 0.9682\n",
      "Epoch 11/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0462 - acc: 0.9900 - val_loss: 0.1953 - val_acc: 0.9582\n",
      "Epoch 12/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0373 - acc: 0.9908 - val_loss: 0.1505 - val_acc: 0.9669\n",
      "Epoch 13/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0390 - acc: 0.9920 - val_loss: 0.1737 - val_acc: 0.9622\n",
      "Epoch 14/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0475 - acc: 0.9905 - val_loss: 0.1543 - val_acc: 0.9652\n",
      "Epoch 15/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0336 - acc: 0.9907 - val_loss: 0.1599 - val_acc: 0.9659\n",
      "Epoch 16/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0531 - acc: 0.9885 - val_loss: 0.1877 - val_acc: 0.9585\n",
      "Epoch 17/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0530 - acc: 0.9892 - val_loss: 0.2258 - val_acc: 0.9512\n",
      "Epoch 18/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0369 - acc: 0.9908 - val_loss: 0.2055 - val_acc: 0.9569\n",
      "Epoch 19/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0487 - acc: 0.9908 - val_loss: 0.1641 - val_acc: 0.9656\n",
      "Epoch 20/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0366 - acc: 0.9902 - val_loss: 0.1724 - val_acc: 0.9622\n",
      "Epoch 21/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0458 - acc: 0.9920 - val_loss: 0.1614 - val_acc: 0.9676\n",
      "Epoch 22/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0469 - acc: 0.9901 - val_loss: 0.1929 - val_acc: 0.9605\n",
      "Epoch 23/500\n",
      "110/109 [==============================] - 47s 431ms/step - loss: 0.0365 - acc: 0.9909 - val_loss: 0.2002 - val_acc: 0.9605\n",
      "Epoch 24/500\n",
      "110/109 [==============================] - 47s 430ms/step - loss: 0.0359 - acc: 0.9935 - val_loss: 0.1851 - val_acc: 0.9615\n",
      "Epoch 25/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0284 - acc: 0.9943 - val_loss: 0.1745 - val_acc: 0.9632\n",
      "Epoch 26/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0489 - acc: 0.9892 - val_loss: 0.2116 - val_acc: 0.9622\n",
      "Epoch 27/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0401 - acc: 0.9910 - val_loss: 0.2162 - val_acc: 0.9599\n",
      "Epoch 28/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0417 - acc: 0.9920 - val_loss: 0.1769 - val_acc: 0.9645\n",
      "Epoch 29/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0347 - acc: 0.9912 - val_loss: 0.1819 - val_acc: 0.9622\n",
      "Epoch 30/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0495 - acc: 0.9929 - val_loss: 0.2104 - val_acc: 0.9612\n",
      "Epoch 31/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0325 - acc: 0.9931 - val_loss: 0.2152 - val_acc: 0.9582\n",
      "Epoch 32/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0275 - acc: 0.9929 - val_loss: 0.2092 - val_acc: 0.9605\n",
      "Epoch 33/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0284 - acc: 0.9940 - val_loss: 0.2074 - val_acc: 0.9569\n",
      "Epoch 34/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0299 - acc: 0.9935 - val_loss: 0.1982 - val_acc: 0.9575\n",
      "Epoch 35/500\n",
      "110/109 [==============================] - 48s 432ms/step - loss: 0.0320 - acc: 0.9937 - val_loss: 0.1721 - val_acc: 0.9656\n",
      "Epoch 36/500\n",
      "110/109 [==============================] - 47s 430ms/step - loss: 0.0347 - acc: 0.9922 - val_loss: 0.1864 - val_acc: 0.9642\n",
      "Epoch 37/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0476 - acc: 0.9903 - val_loss: 0.2231 - val_acc: 0.9605\n",
      "Epoch 38/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0352 - acc: 0.9936 - val_loss: 0.2022 - val_acc: 0.9635\n",
      "Epoch 39/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0289 - acc: 0.9936 - val_loss: 0.1924 - val_acc: 0.9625\n",
      "Epoch 40/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0342 - acc: 0.9939 - val_loss: 0.2183 - val_acc: 0.9609\n",
      "Epoch 41/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0261 - acc: 0.9946 - val_loss: 0.2302 - val_acc: 0.9599\n",
      "Epoch 42/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0229 - acc: 0.9953 - val_loss: 0.1976 - val_acc: 0.9625\n",
      "Epoch 43/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0250 - acc: 0.9949 - val_loss: 0.1945 - val_acc: 0.9635\n",
      "Epoch 44/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0306 - acc: 0.9933 - val_loss: 0.2107 - val_acc: 0.9642\n",
      "Epoch 45/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0450 - acc: 0.9912 - val_loss: 0.1920 - val_acc: 0.9649\n",
      "Epoch 46/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0368 - acc: 0.9911 - val_loss: 0.2772 - val_acc: 0.9575\n",
      "Epoch 47/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0352 - acc: 0.9929 - val_loss: 0.1868 - val_acc: 0.9649\n",
      "Epoch 48/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0364 - acc: 0.9929 - val_loss: 0.2417 - val_acc: 0.9579\n",
      "Epoch 49/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0268 - acc: 0.9948 - val_loss: 0.3084 - val_acc: 0.9472\n",
      "Epoch 50/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0308 - acc: 0.9925 - val_loss: 0.2239 - val_acc: 0.9599\n",
      "Epoch 51/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0483 - acc: 0.9891 - val_loss: 0.2777 - val_acc: 0.9532\n",
      "Epoch 52/500\n",
      "110/109 [==============================] - 47s 430ms/step - loss: 0.0353 - acc: 0.9932 - val_loss: 0.2313 - val_acc: 0.9605\n",
      "Epoch 53/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0345 - acc: 0.9919 - val_loss: 0.2747 - val_acc: 0.9552\n",
      "Epoch 54/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0469 - acc: 0.9911 - val_loss: 0.2202 - val_acc: 0.9602\n",
      "Epoch 55/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0282 - acc: 0.9935 - val_loss: 0.2440 - val_acc: 0.9572\n",
      "Epoch 56/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0295 - acc: 0.9939 - val_loss: 0.2524 - val_acc: 0.9605\n",
      "Epoch 57/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0335 - acc: 0.9937 - val_loss: 0.2719 - val_acc: 0.9559\n",
      "Epoch 58/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0408 - acc: 0.9921 - val_loss: 0.2910 - val_acc: 0.9525\n",
      "Epoch 59/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0293 - acc: 0.9942 - val_loss: 0.2642 - val_acc: 0.9535\n",
      "Epoch 60/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0264 - acc: 0.9939 - val_loss: 0.2885 - val_acc: 0.9532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0330 - acc: 0.9946 - val_loss: 0.2134 - val_acc: 0.9585\n",
      "Epoch 62/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0220 - acc: 0.9945 - val_loss: 0.2605 - val_acc: 0.9605\n",
      "Epoch 63/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0278 - acc: 0.9936 - val_loss: 0.2823 - val_acc: 0.9552\n",
      "Epoch 64/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0298 - acc: 0.9943 - val_loss: 0.2439 - val_acc: 0.9585\n",
      "Epoch 65/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0290 - acc: 0.9924 - val_loss: 0.2430 - val_acc: 0.9582\n",
      "Epoch 66/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0350 - acc: 0.9929 - val_loss: 0.2181 - val_acc: 0.9632\n",
      "Epoch 67/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0240 - acc: 0.9953 - val_loss: 0.2259 - val_acc: 0.9585\n",
      "Epoch 68/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0249 - acc: 0.9949 - val_loss: 0.2297 - val_acc: 0.9555\n",
      "Epoch 69/500\n",
      "110/109 [==============================] - 47s 424ms/step - loss: 0.0266 - acc: 0.9937 - val_loss: 0.2236 - val_acc: 0.9632\n",
      "Epoch 70/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.2722 - val_acc: 0.9542\n",
      "Epoch 71/500\n",
      "110/109 [==============================] - 47s 424ms/step - loss: 0.0247 - acc: 0.9937 - val_loss: 0.2310 - val_acc: 0.9619\n",
      "Epoch 72/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0348 - acc: 0.9932 - val_loss: 0.2607 - val_acc: 0.9612\n",
      "Epoch 73/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0351 - acc: 0.9928 - val_loss: 0.3058 - val_acc: 0.9495\n",
      "Epoch 74/500\n",
      "110/109 [==============================] - 47s 430ms/step - loss: 0.0197 - acc: 0.9943 - val_loss: 0.2558 - val_acc: 0.9562\n",
      "Epoch 75/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0404 - acc: 0.9928 - val_loss: 0.2686 - val_acc: 0.9538\n",
      "Epoch 76/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0290 - acc: 0.9946 - val_loss: 0.3231 - val_acc: 0.9542\n",
      "Epoch 77/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0304 - acc: 0.9941 - val_loss: 0.2710 - val_acc: 0.9592\n",
      "Epoch 78/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0321 - acc: 0.9929 - val_loss: 0.2913 - val_acc: 0.9542\n",
      "Epoch 79/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0239 - acc: 0.9939 - val_loss: 0.2435 - val_acc: 0.9592\n",
      "Epoch 80/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0236 - acc: 0.9949 - val_loss: 0.2768 - val_acc: 0.9525\n",
      "Epoch 81/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0237 - acc: 0.9959 - val_loss: 0.2886 - val_acc: 0.9545\n",
      "Epoch 82/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0267 - acc: 0.9948 - val_loss: 0.2907 - val_acc: 0.9538\n",
      "Epoch 83/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.2860 - val_acc: 0.9532\n",
      "Epoch 84/500\n",
      "110/109 [==============================] - 47s 430ms/step - loss: 0.0318 - acc: 0.9945 - val_loss: 0.2472 - val_acc: 0.9582\n",
      "Epoch 85/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0234 - acc: 0.9952 - val_loss: 0.2864 - val_acc: 0.9522\n",
      "Epoch 86/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0250 - acc: 0.9953 - val_loss: 0.2499 - val_acc: 0.9579\n",
      "Epoch 87/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0253 - acc: 0.9947 - val_loss: 0.2786 - val_acc: 0.9562\n",
      "Epoch 88/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0383 - acc: 0.9924 - val_loss: 0.2487 - val_acc: 0.9605\n",
      "Epoch 89/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0360 - acc: 0.9929 - val_loss: 0.2789 - val_acc: 0.9609\n",
      "Epoch 90/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0307 - acc: 0.9945 - val_loss: 0.3041 - val_acc: 0.9555\n",
      "Epoch 91/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0345 - acc: 0.9927 - val_loss: 0.2536 - val_acc: 0.9555\n",
      "Epoch 92/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0291 - acc: 0.9939 - val_loss: 0.2391 - val_acc: 0.9559\n",
      "Epoch 93/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0281 - acc: 0.9949 - val_loss: 0.3084 - val_acc: 0.9505\n",
      "Epoch 94/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0324 - acc: 0.9925 - val_loss: 0.2354 - val_acc: 0.9589\n",
      "Epoch 95/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0289 - acc: 0.9954 - val_loss: 0.3046 - val_acc: 0.9532\n",
      "Epoch 96/500\n",
      "110/109 [==============================] - 47s 431ms/step - loss: 0.0356 - acc: 0.9934 - val_loss: 0.2991 - val_acc: 0.9488\n",
      "Epoch 97/500\n",
      "110/109 [==============================] - 47s 430ms/step - loss: 0.0255 - acc: 0.9963 - val_loss: 0.3821 - val_acc: 0.9435\n",
      "Epoch 98/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0261 - acc: 0.9947 - val_loss: 0.3330 - val_acc: 0.9465\n",
      "Epoch 99/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0307 - acc: 0.9939 - val_loss: 0.2585 - val_acc: 0.9565\n",
      "Epoch 100/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0327 - acc: 0.9938 - val_loss: 0.2876 - val_acc: 0.9552\n",
      "Epoch 101/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0273 - acc: 0.9946 - val_loss: 0.3490 - val_acc: 0.9508\n",
      "Epoch 102/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0372 - acc: 0.9922 - val_loss: 0.2726 - val_acc: 0.9528\n",
      "Epoch 103/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0297 - acc: 0.9943 - val_loss: 0.2525 - val_acc: 0.9609\n",
      "Epoch 104/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0318 - acc: 0.9933 - val_loss: 0.2982 - val_acc: 0.9542\n",
      "Epoch 105/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0162 - acc: 0.9962 - val_loss: 0.2837 - val_acc: 0.9548\n",
      "Epoch 106/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0362 - acc: 0.9945 - val_loss: 0.3248 - val_acc: 0.9515\n",
      "Epoch 107/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0200 - acc: 0.9955 - val_loss: 0.3455 - val_acc: 0.9488\n",
      "Epoch 108/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0269 - acc: 0.9947 - val_loss: 0.2837 - val_acc: 0.9552\n",
      "Epoch 109/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0316 - acc: 0.9935 - val_loss: 0.3343 - val_acc: 0.9512\n",
      "Epoch 110/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0282 - acc: 0.9955 - val_loss: 0.2484 - val_acc: 0.9592\n",
      "Epoch 111/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0228 - acc: 0.9952 - val_loss: 0.2485 - val_acc: 0.9575\n",
      "Epoch 112/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0265 - acc: 0.9947 - val_loss: 0.2828 - val_acc: 0.9562\n",
      "Epoch 113/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0215 - acc: 0.9959 - val_loss: 0.3177 - val_acc: 0.9495\n",
      "Epoch 114/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0174 - acc: 0.9963 - val_loss: 0.2859 - val_acc: 0.9525\n",
      "Epoch 115/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0300 - acc: 0.9944 - val_loss: 0.2578 - val_acc: 0.9555\n",
      "Epoch 116/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0320 - acc: 0.9942 - val_loss: 0.3253 - val_acc: 0.9522\n",
      "Epoch 117/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0286 - acc: 0.9947 - val_loss: 0.2916 - val_acc: 0.9548\n",
      "Epoch 118/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0306 - acc: 0.9953 - val_loss: 0.2882 - val_acc: 0.9545\n",
      "Epoch 119/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0283 - acc: 0.9945 - val_loss: 0.3275 - val_acc: 0.9505\n",
      "Epoch 120/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0192 - acc: 0.9955 - val_loss: 0.2541 - val_acc: 0.9579\n",
      "Epoch 121/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0288 - acc: 0.9960 - val_loss: 0.2669 - val_acc: 0.9605\n",
      "Epoch 122/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0326 - acc: 0.9933 - val_loss: 0.2785 - val_acc: 0.9572\n",
      "Epoch 123/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0268 - acc: 0.9947 - val_loss: 0.3086 - val_acc: 0.9525\n",
      "Epoch 124/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0266 - acc: 0.9953 - val_loss: 0.2918 - val_acc: 0.9569\n",
      "Epoch 125/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0258 - acc: 0.9960 - val_loss: 0.3217 - val_acc: 0.9525\n",
      "Epoch 126/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0244 - acc: 0.9960 - val_loss: 0.2821 - val_acc: 0.9542\n",
      "Epoch 127/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0297 - acc: 0.9953 - val_loss: 0.2622 - val_acc: 0.9602\n",
      "Epoch 128/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0297 - acc: 0.9942 - val_loss: 0.3114 - val_acc: 0.9538\n",
      "Epoch 129/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0264 - acc: 0.9955 - val_loss: 0.2664 - val_acc: 0.9569\n",
      "Epoch 130/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0205 - acc: 0.9965 - val_loss: 0.3077 - val_acc: 0.9478\n",
      "Epoch 131/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0211 - acc: 0.9957 - val_loss: 0.2468 - val_acc: 0.9595\n",
      "Epoch 132/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0322 - acc: 0.9942 - val_loss: 0.3012 - val_acc: 0.9552\n",
      "Epoch 133/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0188 - acc: 0.9969 - val_loss: 0.3179 - val_acc: 0.9485\n",
      "Epoch 134/500\n",
      "110/109 [==============================] - 47s 430ms/step - loss: 0.0242 - acc: 0.9956 - val_loss: 0.3321 - val_acc: 0.9512\n",
      "Epoch 135/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0186 - acc: 0.9964 - val_loss: 0.3216 - val_acc: 0.9569\n",
      "Epoch 136/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0262 - acc: 0.9943 - val_loss: 0.2816 - val_acc: 0.9585\n",
      "Epoch 137/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0241 - acc: 0.9962 - val_loss: 0.3228 - val_acc: 0.9548\n",
      "Epoch 138/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0154 - acc: 0.9960 - val_loss: 0.3072 - val_acc: 0.9559\n",
      "Epoch 139/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0249 - acc: 0.9960 - val_loss: 0.3175 - val_acc: 0.9562\n",
      "Epoch 140/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0277 - acc: 0.9944 - val_loss: 0.3244 - val_acc: 0.9522\n",
      "Epoch 141/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0196 - acc: 0.9966 - val_loss: 0.3504 - val_acc: 0.9472\n",
      "Epoch 142/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0297 - acc: 0.9946 - val_loss: 0.2912 - val_acc: 0.9542\n",
      "Epoch 143/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0201 - acc: 0.9969 - val_loss: 0.3607 - val_acc: 0.9468\n",
      "Epoch 144/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0240 - acc: 0.9949 - val_loss: 0.3561 - val_acc: 0.9445\n",
      "Epoch 145/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0396 - acc: 0.9941 - val_loss: 0.2792 - val_acc: 0.9532\n",
      "Epoch 146/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0274 - acc: 0.9940 - val_loss: 0.2554 - val_acc: 0.9589\n",
      "Epoch 147/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0283 - acc: 0.9944 - val_loss: 0.2958 - val_acc: 0.9589\n",
      "Epoch 148/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0220 - acc: 0.9964 - val_loss: 0.3473 - val_acc: 0.9505\n",
      "Epoch 149/500\n",
      "110/109 [==============================] - 47s 430ms/step - loss: 0.0392 - acc: 0.9931 - val_loss: 0.3775 - val_acc: 0.9441\n",
      "Epoch 150/500\n",
      "110/109 [==============================] - 48s 433ms/step - loss: 0.0359 - acc: 0.9951 - val_loss: 0.2941 - val_acc: 0.9569\n",
      "Epoch 151/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0209 - acc: 0.9969 - val_loss: 0.2702 - val_acc: 0.9575\n",
      "Epoch 152/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0222 - acc: 0.9957 - val_loss: 0.2761 - val_acc: 0.9575\n",
      "Epoch 153/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0233 - acc: 0.9946 - val_loss: 0.3261 - val_acc: 0.9522\n",
      "Epoch 154/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0310 - acc: 0.9944 - val_loss: 0.2793 - val_acc: 0.9545\n",
      "Epoch 155/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0186 - acc: 0.9962 - val_loss: 0.2969 - val_acc: 0.9535\n",
      "Epoch 156/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0205 - acc: 0.9957 - val_loss: 0.2743 - val_acc: 0.9595\n",
      "Epoch 157/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0197 - acc: 0.9959 - val_loss: 0.3487 - val_acc: 0.9485\n",
      "Epoch 158/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0217 - acc: 0.9963 - val_loss: 0.3036 - val_acc: 0.9572\n",
      "Epoch 159/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0200 - acc: 0.9970 - val_loss: 0.3214 - val_acc: 0.9502\n",
      "Epoch 160/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0319 - acc: 0.9956 - val_loss: 0.3386 - val_acc: 0.9518\n",
      "Epoch 161/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0353 - acc: 0.9928 - val_loss: 0.4214 - val_acc: 0.9401\n",
      "Epoch 162/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0251 - acc: 0.9949 - val_loss: 0.3511 - val_acc: 0.9488\n",
      "Epoch 163/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0214 - acc: 0.9970 - val_loss: 0.3379 - val_acc: 0.9498\n",
      "Epoch 164/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0260 - acc: 0.9949 - val_loss: 0.2798 - val_acc: 0.9569\n",
      "Epoch 165/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0203 - acc: 0.9962 - val_loss: 0.3588 - val_acc: 0.9505\n",
      "Epoch 166/500\n",
      "110/109 [==============================] - 47s 431ms/step - loss: 0.0186 - acc: 0.9969 - val_loss: 0.3809 - val_acc: 0.9505\n",
      "Epoch 167/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0259 - acc: 0.9957 - val_loss: 0.3727 - val_acc: 0.9528\n",
      "Epoch 168/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0160 - acc: 0.9973 - val_loss: 0.3205 - val_acc: 0.9518\n",
      "Epoch 169/500\n",
      "110/109 [==============================] - 47s 424ms/step - loss: 0.0223 - acc: 0.9955 - val_loss: 0.3185 - val_acc: 0.9569\n",
      "Epoch 170/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0248 - acc: 0.9956 - val_loss: 0.3722 - val_acc: 0.9488\n",
      "Epoch 171/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0218 - acc: 0.9963 - val_loss: 0.4485 - val_acc: 0.9435\n",
      "Epoch 172/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0320 - acc: 0.9945 - val_loss: 0.4350 - val_acc: 0.9418\n",
      "Epoch 173/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0284 - acc: 0.9954 - val_loss: 0.3688 - val_acc: 0.9478\n",
      "Epoch 174/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0245 - acc: 0.9960 - val_loss: 0.3652 - val_acc: 0.9478\n",
      "Epoch 175/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0244 - acc: 0.9955 - val_loss: 0.3793 - val_acc: 0.9475\n",
      "Epoch 176/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0277 - acc: 0.9947 - val_loss: 0.3580 - val_acc: 0.9522\n",
      "Epoch 177/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0332 - acc: 0.9939 - val_loss: 0.3636 - val_acc: 0.9495\n",
      "Epoch 178/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0237 - acc: 0.9952 - val_loss: 0.3450 - val_acc: 0.9512\n",
      "Epoch 179/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0276 - acc: 0.9953 - val_loss: 0.3632 - val_acc: 0.9482\n",
      "Epoch 180/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0213 - acc: 0.9963 - val_loss: 0.4265 - val_acc: 0.9388\n",
      "Epoch 181/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0187 - acc: 0.9969 - val_loss: 0.3559 - val_acc: 0.9512\n",
      "Epoch 182/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0207 - acc: 0.9966 - val_loss: 0.3449 - val_acc: 0.9522\n",
      "Epoch 183/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0220 - acc: 0.9963 - val_loss: 0.3317 - val_acc: 0.9562\n",
      "Epoch 184/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0166 - acc: 0.9979 - val_loss: 0.3756 - val_acc: 0.9522\n",
      "Epoch 185/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0128 - acc: 0.9974 - val_loss: 0.3953 - val_acc: 0.9468\n",
      "Epoch 186/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0320 - acc: 0.9942 - val_loss: 0.4253 - val_acc: 0.9425\n",
      "Epoch 187/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0289 - acc: 0.9962 - val_loss: 0.4612 - val_acc: 0.9445\n",
      "Epoch 188/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0264 - acc: 0.9966 - val_loss: 0.4015 - val_acc: 0.9475\n",
      "Epoch 189/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0211 - acc: 0.9970 - val_loss: 0.3770 - val_acc: 0.9515\n",
      "Epoch 190/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0120 - acc: 0.9972 - val_loss: 0.3624 - val_acc: 0.9545\n",
      "Epoch 191/500\n",
      "110/109 [==============================] - 47s 431ms/step - loss: 0.0254 - acc: 0.9958 - val_loss: 0.3452 - val_acc: 0.9538\n",
      "Epoch 192/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0234 - acc: 0.9963 - val_loss: 0.3958 - val_acc: 0.9458\n",
      "Epoch 193/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0194 - acc: 0.9962 - val_loss: 0.3922 - val_acc: 0.9462\n",
      "Epoch 194/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0313 - acc: 0.9938 - val_loss: 0.3354 - val_acc: 0.9582\n",
      "Epoch 195/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0315 - acc: 0.9943 - val_loss: 0.3970 - val_acc: 0.9475\n",
      "Epoch 196/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0224 - acc: 0.9950 - val_loss: 0.3617 - val_acc: 0.9525\n",
      "Epoch 197/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0163 - acc: 0.9960 - val_loss: 0.3734 - val_acc: 0.9518\n",
      "Epoch 198/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0199 - acc: 0.9960 - val_loss: 0.3883 - val_acc: 0.9505\n",
      "Epoch 199/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0379 - acc: 0.9958 - val_loss: 0.4105 - val_acc: 0.9448\n",
      "Epoch 200/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0271 - acc: 0.9964 - val_loss: 0.3827 - val_acc: 0.9485\n",
      "Epoch 201/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0207 - acc: 0.9957 - val_loss: 0.3656 - val_acc: 0.9522\n",
      "Epoch 202/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0268 - acc: 0.9957 - val_loss: 0.3733 - val_acc: 0.9495\n",
      "Epoch 203/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0343 - acc: 0.9931 - val_loss: 0.4573 - val_acc: 0.9438\n",
      "Epoch 204/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0324 - acc: 0.9943 - val_loss: 0.4495 - val_acc: 0.9495\n",
      "Epoch 205/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0498 - acc: 0.9920 - val_loss: 0.3323 - val_acc: 0.9582\n",
      "Epoch 206/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0312 - acc: 0.9945 - val_loss: 0.3483 - val_acc: 0.9559\n",
      "Epoch 207/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0298 - acc: 0.9955 - val_loss: 0.4162 - val_acc: 0.9472\n",
      "Epoch 208/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0196 - acc: 0.9963 - val_loss: 0.4516 - val_acc: 0.9431\n",
      "Epoch 209/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0261 - acc: 0.9955 - val_loss: 0.4457 - val_acc: 0.9448\n",
      "Epoch 210/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0140 - acc: 0.9963 - val_loss: 0.4039 - val_acc: 0.9488\n",
      "Epoch 211/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0363 - acc: 0.9959 - val_loss: 0.3742 - val_acc: 0.9542\n",
      "Epoch 212/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0274 - acc: 0.9948 - val_loss: 0.4260 - val_acc: 0.9482\n",
      "Epoch 213/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0239 - acc: 0.9956 - val_loss: 0.3510 - val_acc: 0.9565\n",
      "Epoch 214/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0141 - acc: 0.9977 - val_loss: 0.3972 - val_acc: 0.9475\n",
      "Epoch 215/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0241 - acc: 0.9960 - val_loss: 0.3487 - val_acc: 0.9528\n",
      "Epoch 216/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0132 - acc: 0.9966 - val_loss: 0.3432 - val_acc: 0.9535\n",
      "Epoch 217/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0328 - acc: 0.9959 - val_loss: 0.4108 - val_acc: 0.9472\n",
      "Epoch 218/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0314 - acc: 0.9949 - val_loss: 0.4084 - val_acc: 0.9532\n",
      "Epoch 219/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0156 - acc: 0.9969 - val_loss: 0.3426 - val_acc: 0.9555\n",
      "Epoch 220/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0376 - acc: 0.9937 - val_loss: 0.3573 - val_acc: 0.9542\n",
      "Epoch 221/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0227 - acc: 0.9963 - val_loss: 0.3830 - val_acc: 0.9542\n",
      "Epoch 222/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0188 - acc: 0.9969 - val_loss: 0.3439 - val_acc: 0.9512\n",
      "Epoch 223/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0248 - acc: 0.9960 - val_loss: 0.4102 - val_acc: 0.9482\n",
      "Epoch 224/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0229 - acc: 0.9972 - val_loss: 0.4171 - val_acc: 0.9485\n",
      "Epoch 225/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0157 - acc: 0.9968 - val_loss: 0.3497 - val_acc: 0.9512\n",
      "Epoch 226/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0238 - acc: 0.9974 - val_loss: 0.4345 - val_acc: 0.9468\n",
      "Epoch 227/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0216 - acc: 0.9962 - val_loss: 0.3717 - val_acc: 0.9522\n",
      "Epoch 228/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0188 - acc: 0.9973 - val_loss: 0.3268 - val_acc: 0.9562\n",
      "Epoch 229/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0204 - acc: 0.9966 - val_loss: 0.3693 - val_acc: 0.9535\n",
      "Epoch 230/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0330 - acc: 0.9950 - val_loss: 0.3166 - val_acc: 0.9582\n",
      "Epoch 231/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0258 - acc: 0.9950 - val_loss: 0.3393 - val_acc: 0.9559\n",
      "Epoch 232/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0292 - acc: 0.9959 - val_loss: 0.3887 - val_acc: 0.9552\n",
      "Epoch 233/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0261 - acc: 0.9957 - val_loss: 0.3421 - val_acc: 0.9552\n",
      "Epoch 234/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0283 - acc: 0.9961 - val_loss: 0.3524 - val_acc: 0.9552\n",
      "Epoch 235/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0251 - acc: 0.9960 - val_loss: 0.3098 - val_acc: 0.9599\n",
      "Epoch 236/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0235 - acc: 0.9969 - val_loss: 0.3474 - val_acc: 0.9575\n",
      "Epoch 237/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0292 - acc: 0.9949 - val_loss: 0.3988 - val_acc: 0.9462\n",
      "Epoch 238/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0259 - acc: 0.9954 - val_loss: 0.3922 - val_acc: 0.9498\n",
      "Epoch 239/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0233 - acc: 0.9959 - val_loss: 0.3888 - val_acc: 0.9532\n",
      "Epoch 240/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0198 - acc: 0.9969 - val_loss: 0.4145 - val_acc: 0.9478\n",
      "Epoch 241/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0254 - acc: 0.9964 - val_loss: 0.4363 - val_acc: 0.9465\n",
      "Epoch 242/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0160 - acc: 0.9969 - val_loss: 0.3753 - val_acc: 0.9545\n",
      "Epoch 243/500\n",
      "110/109 [==============================] - 47s 431ms/step - loss: 0.0284 - acc: 0.9950 - val_loss: 0.4206 - val_acc: 0.9515\n",
      "Epoch 244/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0334 - acc: 0.9955 - val_loss: 0.4384 - val_acc: 0.9488\n",
      "Epoch 245/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0214 - acc: 0.9972 - val_loss: 0.3683 - val_acc: 0.9525\n",
      "Epoch 246/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0293 - acc: 0.9960 - val_loss: 0.3943 - val_acc: 0.9482\n",
      "Epoch 247/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0199 - acc: 0.9956 - val_loss: 0.3722 - val_acc: 0.9545\n",
      "Epoch 248/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0208 - acc: 0.9967 - val_loss: 0.3506 - val_acc: 0.9579\n",
      "Epoch 249/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0212 - acc: 0.9967 - val_loss: 0.3481 - val_acc: 0.9592\n",
      "Epoch 250/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0210 - acc: 0.9963 - val_loss: 0.4646 - val_acc: 0.9438\n",
      "Epoch 251/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0328 - acc: 0.9952 - val_loss: 0.4335 - val_acc: 0.9495\n",
      "Epoch 252/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0172 - acc: 0.9974 - val_loss: 0.3604 - val_acc: 0.9548\n",
      "Epoch 253/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0240 - acc: 0.9967 - val_loss: 0.4458 - val_acc: 0.9502\n",
      "Epoch 254/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0273 - acc: 0.9960 - val_loss: 0.4169 - val_acc: 0.9475\n",
      "Epoch 255/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0280 - acc: 0.9956 - val_loss: 0.3906 - val_acc: 0.9495\n",
      "Epoch 256/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0200 - acc: 0.9964 - val_loss: 0.3944 - val_acc: 0.9502\n",
      "Epoch 257/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0224 - acc: 0.9964 - val_loss: 0.4330 - val_acc: 0.9438\n",
      "Epoch 258/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0233 - acc: 0.9960 - val_loss: 0.3469 - val_acc: 0.9548\n",
      "Epoch 259/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0225 - acc: 0.9962 - val_loss: 0.3583 - val_acc: 0.9542\n",
      "Epoch 260/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0296 - acc: 0.9951 - val_loss: 0.4404 - val_acc: 0.9492\n",
      "Epoch 261/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0190 - acc: 0.9969 - val_loss: 0.4091 - val_acc: 0.9512\n",
      "Epoch 262/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0232 - acc: 0.9955 - val_loss: 0.3927 - val_acc: 0.9542\n",
      "Epoch 263/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0258 - acc: 0.9959 - val_loss: 0.3902 - val_acc: 0.9515\n",
      "Epoch 264/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0271 - acc: 0.9959 - val_loss: 0.3492 - val_acc: 0.9565\n",
      "Epoch 265/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0180 - acc: 0.9971 - val_loss: 0.4103 - val_acc: 0.9518\n",
      "Epoch 266/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0140 - acc: 0.9982 - val_loss: 0.3618 - val_acc: 0.9548\n",
      "Epoch 267/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0172 - acc: 0.9970 - val_loss: 0.3918 - val_acc: 0.9569\n",
      "Epoch 268/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0188 - acc: 0.9969 - val_loss: 0.3694 - val_acc: 0.9538\n",
      "Epoch 269/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0234 - acc: 0.9963 - val_loss: 0.3929 - val_acc: 0.9492\n",
      "Epoch 270/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0263 - acc: 0.9970 - val_loss: 0.4161 - val_acc: 0.9482\n",
      "Epoch 271/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0233 - acc: 0.9956 - val_loss: 0.4028 - val_acc: 0.9498\n",
      "Epoch 272/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0179 - acc: 0.9976 - val_loss: 0.3766 - val_acc: 0.9522\n",
      "Epoch 273/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0201 - acc: 0.9969 - val_loss: 0.3652 - val_acc: 0.9532\n",
      "Epoch 274/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0191 - acc: 0.9970 - val_loss: 0.3510 - val_acc: 0.9545\n",
      "Epoch 275/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0194 - acc: 0.9967 - val_loss: 0.3382 - val_acc: 0.9542\n",
      "Epoch 276/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0243 - acc: 0.9972 - val_loss: 0.3951 - val_acc: 0.9512\n",
      "Epoch 277/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0294 - acc: 0.9967 - val_loss: 0.4627 - val_acc: 0.9405\n",
      "Epoch 278/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0208 - acc: 0.9963 - val_loss: 0.4223 - val_acc: 0.9465\n",
      "Epoch 279/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0306 - acc: 0.9959 - val_loss: 0.4010 - val_acc: 0.9495\n",
      "Epoch 280/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0251 - acc: 0.9945 - val_loss: 0.3618 - val_acc: 0.9525\n",
      "Epoch 281/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0294 - acc: 0.9957 - val_loss: 0.5169 - val_acc: 0.9405\n",
      "Epoch 282/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0207 - acc: 0.9969 - val_loss: 0.3795 - val_acc: 0.9538\n",
      "Epoch 283/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0216 - acc: 0.9964 - val_loss: 0.4598 - val_acc: 0.9475\n",
      "Epoch 284/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0256 - acc: 0.9956 - val_loss: 0.3706 - val_acc: 0.9545\n",
      "Epoch 285/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0114 - acc: 0.9980 - val_loss: 0.4286 - val_acc: 0.9478\n",
      "Epoch 286/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0274 - acc: 0.9957 - val_loss: 0.4261 - val_acc: 0.9502\n",
      "Epoch 287/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0320 - acc: 0.9962 - val_loss: 0.3685 - val_acc: 0.9559\n",
      "Epoch 288/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0444 - acc: 0.9938 - val_loss: 0.3524 - val_acc: 0.9575\n",
      "Epoch 289/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0294 - acc: 0.9940 - val_loss: 0.3115 - val_acc: 0.9589\n",
      "Epoch 290/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0213 - acc: 0.9974 - val_loss: 0.4023 - val_acc: 0.9522\n",
      "Epoch 291/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0186 - acc: 0.9972 - val_loss: 0.3493 - val_acc: 0.9585\n",
      "Epoch 292/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0175 - acc: 0.9966 - val_loss: 0.4101 - val_acc: 0.9525\n",
      "Epoch 293/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0282 - acc: 0.9967 - val_loss: 0.4296 - val_acc: 0.9485\n",
      "Epoch 294/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0306 - acc: 0.9957 - val_loss: 0.4066 - val_acc: 0.9562\n",
      "Epoch 295/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0172 - acc: 0.9970 - val_loss: 0.3961 - val_acc: 0.9532\n",
      "Epoch 296/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0207 - acc: 0.9974 - val_loss: 0.3246 - val_acc: 0.9585\n",
      "Epoch 297/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0303 - acc: 0.9956 - val_loss: 0.3921 - val_acc: 0.9535\n",
      "Epoch 298/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0248 - acc: 0.9951 - val_loss: 0.4752 - val_acc: 0.9441\n",
      "Epoch 299/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0349 - acc: 0.9959 - val_loss: 0.4559 - val_acc: 0.9472\n",
      "Epoch 300/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0309 - acc: 0.9960 - val_loss: 0.4146 - val_acc: 0.9478\n",
      "Epoch 301/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0231 - acc: 0.9966 - val_loss: 0.3746 - val_acc: 0.9525\n",
      "Epoch 302/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0206 - acc: 0.9964 - val_loss: 0.3914 - val_acc: 0.9542\n",
      "Epoch 303/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0184 - acc: 0.9970 - val_loss: 0.3606 - val_acc: 0.9535\n",
      "Epoch 304/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0215 - acc: 0.9977 - val_loss: 0.3971 - val_acc: 0.9502\n",
      "Epoch 305/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0171 - acc: 0.9969 - val_loss: 0.4524 - val_acc: 0.9485\n",
      "Epoch 306/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0167 - acc: 0.9976 - val_loss: 0.3938 - val_acc: 0.9535\n",
      "Epoch 307/500\n",
      "110/109 [==============================] - 47s 424ms/step - loss: 0.0283 - acc: 0.9957 - val_loss: 0.4121 - val_acc: 0.9498\n",
      "Epoch 308/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0180 - acc: 0.9972 - val_loss: 0.4872 - val_acc: 0.9411\n",
      "Epoch 309/500\n",
      "110/109 [==============================] - 47s 424ms/step - loss: 0.0245 - acc: 0.9959 - val_loss: 0.3831 - val_acc: 0.9512\n",
      "Epoch 310/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0118 - acc: 0.9982 - val_loss: 0.4336 - val_acc: 0.9522\n",
      "Epoch 311/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0287 - acc: 0.9964 - val_loss: 0.4574 - val_acc: 0.9472\n",
      "Epoch 312/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0300 - acc: 0.9953 - val_loss: 0.5046 - val_acc: 0.9431\n",
      "Epoch 313/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0178 - acc: 0.9974 - val_loss: 0.4501 - val_acc: 0.9431\n",
      "Epoch 314/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0229 - acc: 0.9970 - val_loss: 0.4441 - val_acc: 0.9448\n",
      "Epoch 315/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0306 - acc: 0.9964 - val_loss: 0.4100 - val_acc: 0.9522\n",
      "Epoch 316/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0089 - acc: 0.9986 - val_loss: 0.4299 - val_acc: 0.9498\n",
      "Epoch 317/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0243 - acc: 0.9970 - val_loss: 0.4229 - val_acc: 0.9522\n",
      "Epoch 318/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0281 - acc: 0.9966 - val_loss: 0.4081 - val_acc: 0.9535\n",
      "Epoch 319/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0143 - acc: 0.9977 - val_loss: 0.3824 - val_acc: 0.9572\n",
      "Epoch 320/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0324 - acc: 0.9959 - val_loss: 0.3905 - val_acc: 0.9562\n",
      "Epoch 321/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0239 - acc: 0.9963 - val_loss: 0.5144 - val_acc: 0.9445\n",
      "Epoch 322/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0240 - acc: 0.9966 - val_loss: 0.5161 - val_acc: 0.9421\n",
      "Epoch 323/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0225 - acc: 0.9966 - val_loss: 0.4327 - val_acc: 0.9518\n",
      "Epoch 324/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0401 - acc: 0.9947 - val_loss: 0.4856 - val_acc: 0.9448\n",
      "Epoch 325/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0255 - acc: 0.9970 - val_loss: 0.4919 - val_acc: 0.9455\n",
      "Epoch 326/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0120 - acc: 0.9979 - val_loss: 0.4030 - val_acc: 0.9548\n",
      "Epoch 327/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0199 - acc: 0.9972 - val_loss: 0.4272 - val_acc: 0.9492\n",
      "Epoch 328/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0216 - acc: 0.9962 - val_loss: 0.5915 - val_acc: 0.9341\n",
      "Epoch 329/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0172 - acc: 0.9979 - val_loss: 0.4616 - val_acc: 0.9468\n",
      "Epoch 330/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0166 - acc: 0.9974 - val_loss: 0.4929 - val_acc: 0.9435\n",
      "Epoch 331/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0140 - acc: 0.9976 - val_loss: 0.4522 - val_acc: 0.9452\n",
      "Epoch 332/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0263 - acc: 0.9956 - val_loss: 0.5505 - val_acc: 0.9388\n",
      "Epoch 333/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0367 - acc: 0.9954 - val_loss: 0.5090 - val_acc: 0.9468\n",
      "Epoch 334/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0272 - acc: 0.9953 - val_loss: 0.5032 - val_acc: 0.9435\n",
      "Epoch 335/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0232 - acc: 0.9967 - val_loss: 0.4230 - val_acc: 0.9535\n",
      "Epoch 336/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0219 - acc: 0.9963 - val_loss: 0.4415 - val_acc: 0.9518\n",
      "Epoch 337/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0179 - acc: 0.9979 - val_loss: 0.4548 - val_acc: 0.9498\n",
      "Epoch 338/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0224 - acc: 0.9964 - val_loss: 0.4620 - val_acc: 0.9495\n",
      "Epoch 339/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0192 - acc: 0.9976 - val_loss: 0.4308 - val_acc: 0.9525\n",
      "Epoch 340/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0202 - acc: 0.9966 - val_loss: 0.4752 - val_acc: 0.9482\n",
      "Epoch 341/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0261 - acc: 0.9959 - val_loss: 0.4541 - val_acc: 0.9478\n",
      "Epoch 342/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0188 - acc: 0.9974 - val_loss: 0.4461 - val_acc: 0.9468\n",
      "Epoch 343/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0229 - acc: 0.9962 - val_loss: 0.5140 - val_acc: 0.9395\n",
      "Epoch 344/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0405 - acc: 0.9941 - val_loss: 0.3950 - val_acc: 0.9552\n",
      "Epoch 345/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0196 - acc: 0.9956 - val_loss: 0.4248 - val_acc: 0.9472\n",
      "Epoch 346/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0173 - acc: 0.9970 - val_loss: 0.4547 - val_acc: 0.9475\n",
      "Epoch 347/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0228 - acc: 0.9976 - val_loss: 0.4151 - val_acc: 0.9532\n",
      "Epoch 348/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0148 - acc: 0.9973 - val_loss: 0.4189 - val_acc: 0.9488\n",
      "Epoch 349/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0304 - acc: 0.9971 - val_loss: 0.4375 - val_acc: 0.9495\n",
      "Epoch 350/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0264 - acc: 0.9970 - val_loss: 0.4044 - val_acc: 0.9502\n",
      "Epoch 351/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0168 - acc: 0.9980 - val_loss: 0.4577 - val_acc: 0.9485\n",
      "Epoch 352/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0213 - acc: 0.9963 - val_loss: 0.4496 - val_acc: 0.9502\n",
      "Epoch 353/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0236 - acc: 0.9972 - val_loss: 0.4154 - val_acc: 0.9548\n",
      "Epoch 354/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0181 - acc: 0.9966 - val_loss: 0.5109 - val_acc: 0.9368\n",
      "Epoch 355/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0291 - acc: 0.9962 - val_loss: 0.5079 - val_acc: 0.9405\n",
      "Epoch 356/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0187 - acc: 0.9976 - val_loss: 0.4829 - val_acc: 0.9435\n",
      "Epoch 357/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0217 - acc: 0.9973 - val_loss: 0.5053 - val_acc: 0.9448\n",
      "Epoch 358/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0252 - acc: 0.9963 - val_loss: 0.4339 - val_acc: 0.9498\n",
      "Epoch 359/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0174 - acc: 0.9977 - val_loss: 0.3880 - val_acc: 0.9502\n",
      "Epoch 360/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0167 - acc: 0.9980 - val_loss: 0.4250 - val_acc: 0.9505\n",
      "Epoch 361/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0173 - acc: 0.9977 - val_loss: 0.3668 - val_acc: 0.9579\n",
      "Epoch 362/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0109 - acc: 0.9980 - val_loss: 0.5009 - val_acc: 0.9498\n",
      "Epoch 363/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0133 - acc: 0.9983 - val_loss: 0.4523 - val_acc: 0.9502\n",
      "Epoch 364/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0184 - acc: 0.9979 - val_loss: 0.4589 - val_acc: 0.9502\n",
      "Epoch 365/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0253 - acc: 0.9969 - val_loss: 0.4200 - val_acc: 0.9525\n",
      "Epoch 366/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0156 - acc: 0.9974 - val_loss: 0.5359 - val_acc: 0.9428\n",
      "Epoch 367/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0221 - acc: 0.9972 - val_loss: 0.4436 - val_acc: 0.9465\n",
      "Epoch 368/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0310 - acc: 0.9959 - val_loss: 0.3996 - val_acc: 0.9545\n",
      "Epoch 369/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0221 - acc: 0.9974 - val_loss: 0.4840 - val_acc: 0.9468\n",
      "Epoch 370/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0321 - acc: 0.9951 - val_loss: 0.4967 - val_acc: 0.9465\n",
      "Epoch 371/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0188 - acc: 0.9972 - val_loss: 0.4588 - val_acc: 0.9545\n",
      "Epoch 372/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0230 - acc: 0.9967 - val_loss: 0.4412 - val_acc: 0.9535\n",
      "Epoch 373/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0244 - acc: 0.9959 - val_loss: 0.3771 - val_acc: 0.9562\n",
      "Epoch 374/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0153 - acc: 0.9972 - val_loss: 0.5042 - val_acc: 0.9482\n",
      "Epoch 375/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0399 - acc: 0.9948 - val_loss: 0.5864 - val_acc: 0.9371\n",
      "Epoch 376/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0493 - acc: 0.9927 - val_loss: 0.4672 - val_acc: 0.9518\n",
      "Epoch 377/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0194 - acc: 0.9971 - val_loss: 0.4494 - val_acc: 0.9495\n",
      "Epoch 378/500\n",
      "110/109 [==============================] - 47s 430ms/step - loss: 0.0359 - acc: 0.9948 - val_loss: 0.4979 - val_acc: 0.9498\n",
      "Epoch 379/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0349 - acc: 0.9954 - val_loss: 0.5635 - val_acc: 0.9421\n",
      "Epoch 380/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0208 - acc: 0.9969 - val_loss: 0.4849 - val_acc: 0.9485\n",
      "Epoch 381/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0325 - acc: 0.9954 - val_loss: 0.4541 - val_acc: 0.9498\n",
      "Epoch 382/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0223 - acc: 0.9962 - val_loss: 0.4722 - val_acc: 0.9495\n",
      "Epoch 383/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0221 - acc: 0.9970 - val_loss: 0.4040 - val_acc: 0.9535\n",
      "Epoch 384/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0208 - acc: 0.9972 - val_loss: 0.4406 - val_acc: 0.9508\n",
      "Epoch 385/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0139 - acc: 0.9978 - val_loss: 0.4848 - val_acc: 0.9468\n",
      "Epoch 386/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0205 - acc: 0.9977 - val_loss: 0.4123 - val_acc: 0.9572\n",
      "Epoch 387/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0161 - acc: 0.9970 - val_loss: 0.4706 - val_acc: 0.9498\n",
      "Epoch 388/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0158 - acc: 0.9972 - val_loss: 0.4558 - val_acc: 0.9515\n",
      "Epoch 389/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0225 - acc: 0.9964 - val_loss: 0.4611 - val_acc: 0.9475\n",
      "Epoch 390/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0224 - acc: 0.9966 - val_loss: 0.5082 - val_acc: 0.9428\n",
      "Epoch 391/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0354 - acc: 0.9952 - val_loss: 0.4096 - val_acc: 0.9542\n",
      "Epoch 392/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0198 - acc: 0.9972 - val_loss: 0.4522 - val_acc: 0.9552\n",
      "Epoch 393/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0221 - acc: 0.9964 - val_loss: 0.4937 - val_acc: 0.9492\n",
      "Epoch 394/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0260 - acc: 0.9959 - val_loss: 0.4557 - val_acc: 0.9468\n",
      "Epoch 395/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0187 - acc: 0.9977 - val_loss: 0.4820 - val_acc: 0.9472\n",
      "Epoch 396/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0223 - acc: 0.9964 - val_loss: 0.5483 - val_acc: 0.9448\n",
      "Epoch 397/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0211 - acc: 0.9974 - val_loss: 0.4968 - val_acc: 0.9472\n",
      "Epoch 398/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0289 - acc: 0.9966 - val_loss: 0.4640 - val_acc: 0.9508\n",
      "Epoch 399/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0248 - acc: 0.9972 - val_loss: 0.3869 - val_acc: 0.9555\n",
      "Epoch 400/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0136 - acc: 0.9982 - val_loss: 0.4339 - val_acc: 0.9522\n",
      "Epoch 401/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0192 - acc: 0.9982 - val_loss: 0.4346 - val_acc: 0.9562\n",
      "Epoch 402/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0174 - acc: 0.9973 - val_loss: 0.4122 - val_acc: 0.9538\n",
      "Epoch 403/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0231 - acc: 0.9972 - val_loss: 0.4255 - val_acc: 0.9538\n",
      "Epoch 404/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0203 - acc: 0.9976 - val_loss: 0.4459 - val_acc: 0.9492\n",
      "Epoch 405/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0229 - acc: 0.9975 - val_loss: 0.5115 - val_acc: 0.9448\n",
      "Epoch 406/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0386 - acc: 0.9949 - val_loss: 0.4938 - val_acc: 0.9485\n",
      "Epoch 407/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0267 - acc: 0.9967 - val_loss: 0.4754 - val_acc: 0.9505\n",
      "Epoch 408/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0209 - acc: 0.9966 - val_loss: 0.4206 - val_acc: 0.9562\n",
      "Epoch 409/500\n",
      "110/109 [==============================] - 47s 424ms/step - loss: 0.0181 - acc: 0.9972 - val_loss: 0.4453 - val_acc: 0.9505\n",
      "Epoch 410/500\n",
      "110/109 [==============================] - 47s 430ms/step - loss: 0.0241 - acc: 0.9972 - val_loss: 0.3921 - val_acc: 0.9565\n",
      "Epoch 411/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0285 - acc: 0.9966 - val_loss: 0.4221 - val_acc: 0.9562\n",
      "Epoch 412/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0306 - acc: 0.9966 - val_loss: 0.4741 - val_acc: 0.9468\n",
      "Epoch 413/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0232 - acc: 0.9967 - val_loss: 0.4414 - val_acc: 0.9478\n",
      "Epoch 414/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0167 - acc: 0.9970 - val_loss: 0.3639 - val_acc: 0.9609\n",
      "Epoch 415/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0356 - acc: 0.9955 - val_loss: 0.3559 - val_acc: 0.9609\n",
      "Epoch 416/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0160 - acc: 0.9976 - val_loss: 0.4544 - val_acc: 0.9482\n",
      "Epoch 417/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0330 - acc: 0.9954 - val_loss: 0.4891 - val_acc: 0.9448\n",
      "Epoch 418/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0304 - acc: 0.9961 - val_loss: 0.5190 - val_acc: 0.9462\n",
      "Epoch 419/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0265 - acc: 0.9963 - val_loss: 0.3988 - val_acc: 0.9535\n",
      "Epoch 420/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0348 - acc: 0.9961 - val_loss: 0.4782 - val_acc: 0.9485\n",
      "Epoch 421/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0247 - acc: 0.9969 - val_loss: 0.4920 - val_acc: 0.9465\n",
      "Epoch 422/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0291 - acc: 0.9964 - val_loss: 0.4880 - val_acc: 0.9468\n",
      "Epoch 423/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0184 - acc: 0.9973 - val_loss: 0.3874 - val_acc: 0.9542\n",
      "Epoch 424/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0161 - acc: 0.9972 - val_loss: 0.5097 - val_acc: 0.9448\n",
      "Epoch 425/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0167 - acc: 0.9974 - val_loss: 0.4764 - val_acc: 0.9508\n",
      "Epoch 426/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0218 - acc: 0.9976 - val_loss: 0.4457 - val_acc: 0.9515\n",
      "Epoch 427/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0118 - acc: 0.9983 - val_loss: 0.4505 - val_acc: 0.9515\n",
      "Epoch 428/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0223 - acc: 0.9967 - val_loss: 0.4742 - val_acc: 0.9488\n",
      "Epoch 429/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0194 - acc: 0.9969 - val_loss: 0.4830 - val_acc: 0.9465\n",
      "Epoch 430/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0271 - acc: 0.9964 - val_loss: 0.5084 - val_acc: 0.9425\n",
      "Epoch 431/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0246 - acc: 0.9970 - val_loss: 0.4672 - val_acc: 0.9472\n",
      "Epoch 432/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0167 - acc: 0.9977 - val_loss: 0.4379 - val_acc: 0.9505\n",
      "Epoch 433/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0274 - acc: 0.9955 - val_loss: 0.5823 - val_acc: 0.9365\n",
      "Epoch 434/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0267 - acc: 0.9965 - val_loss: 0.4424 - val_acc: 0.9515\n",
      "Epoch 435/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0176 - acc: 0.9980 - val_loss: 0.4552 - val_acc: 0.9488\n",
      "Epoch 436/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0158 - acc: 0.9975 - val_loss: 0.5147 - val_acc: 0.9441\n",
      "Epoch 437/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0197 - acc: 0.9977 - val_loss: 0.4443 - val_acc: 0.9525\n",
      "Epoch 438/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0228 - acc: 0.9972 - val_loss: 0.4332 - val_acc: 0.9525\n",
      "Epoch 439/500\n",
      "110/109 [==============================] - 47s 424ms/step - loss: 0.0203 - acc: 0.9972 - val_loss: 0.4328 - val_acc: 0.9559\n",
      "Epoch 440/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0208 - acc: 0.9971 - val_loss: 0.5314 - val_acc: 0.9435\n",
      "Epoch 441/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0246 - acc: 0.9966 - val_loss: 0.4502 - val_acc: 0.9488\n",
      "Epoch 442/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0149 - acc: 0.9974 - val_loss: 0.4770 - val_acc: 0.9448\n",
      "Epoch 443/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0221 - acc: 0.9980 - val_loss: 0.5064 - val_acc: 0.9431\n",
      "Epoch 444/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0180 - acc: 0.9968 - val_loss: 0.4400 - val_acc: 0.9535\n",
      "Epoch 445/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0192 - acc: 0.9980 - val_loss: 0.4437 - val_acc: 0.9542\n",
      "Epoch 446/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0155 - acc: 0.9973 - val_loss: 0.4638 - val_acc: 0.9488\n",
      "Epoch 447/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0212 - acc: 0.9976 - val_loss: 0.4530 - val_acc: 0.9512\n",
      "Epoch 448/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0133 - acc: 0.9986 - val_loss: 0.4544 - val_acc: 0.9495\n",
      "Epoch 449/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0145 - acc: 0.9982 - val_loss: 0.5081 - val_acc: 0.9455\n",
      "Epoch 450/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0152 - acc: 0.9986 - val_loss: 0.4681 - val_acc: 0.9458\n",
      "Epoch 451/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0103 - acc: 0.9980 - val_loss: 0.4693 - val_acc: 0.9518\n",
      "Epoch 452/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0169 - acc: 0.9982 - val_loss: 0.4787 - val_acc: 0.9478\n",
      "Epoch 453/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0195 - acc: 0.9974 - val_loss: 0.5634 - val_acc: 0.9415\n",
      "Epoch 454/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0118 - acc: 0.9986 - val_loss: 0.5079 - val_acc: 0.9448\n",
      "Epoch 455/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0229 - acc: 0.9977 - val_loss: 0.6239 - val_acc: 0.9355\n",
      "Epoch 456/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0170 - acc: 0.9969 - val_loss: 0.5473 - val_acc: 0.9425\n",
      "Epoch 457/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0271 - acc: 0.9969 - val_loss: 0.5432 - val_acc: 0.9411\n",
      "Epoch 458/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0249 - acc: 0.9972 - val_loss: 0.5291 - val_acc: 0.9435\n",
      "Epoch 459/500\n",
      "110/109 [==============================] - 47s 424ms/step - loss: 0.0181 - acc: 0.9974 - val_loss: 0.4926 - val_acc: 0.9475\n",
      "Epoch 460/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0474 - acc: 0.9945 - val_loss: 0.7200 - val_acc: 0.9288\n",
      "Epoch 461/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0318 - acc: 0.9960 - val_loss: 0.6382 - val_acc: 0.9318\n",
      "Epoch 462/500\n",
      "110/109 [==============================] - 47s 429ms/step - loss: 0.0241 - acc: 0.9967 - val_loss: 0.5572 - val_acc: 0.9408\n",
      "Epoch 463/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0334 - acc: 0.9951 - val_loss: 0.7985 - val_acc: 0.9211\n",
      "Epoch 464/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0317 - acc: 0.9960 - val_loss: 0.5864 - val_acc: 0.9405\n",
      "Epoch 465/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0340 - acc: 0.9955 - val_loss: 0.5887 - val_acc: 0.9411\n",
      "Epoch 466/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0312 - acc: 0.9950 - val_loss: 0.6111 - val_acc: 0.9388\n",
      "Epoch 467/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0218 - acc: 0.9974 - val_loss: 0.5496 - val_acc: 0.9421\n",
      "Epoch 468/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0201 - acc: 0.9972 - val_loss: 0.5790 - val_acc: 0.9375\n",
      "Epoch 469/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0212 - acc: 0.9972 - val_loss: 0.5246 - val_acc: 0.9438\n",
      "Epoch 470/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0167 - acc: 0.9980 - val_loss: 0.5597 - val_acc: 0.9405\n",
      "Epoch 471/500\n",
      "110/109 [==============================] - 47s 432ms/step - loss: 0.0256 - acc: 0.9973 - val_loss: 0.5484 - val_acc: 0.9455\n",
      "Epoch 472/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0255 - acc: 0.9962 - val_loss: 0.5385 - val_acc: 0.9418\n",
      "Epoch 473/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0296 - acc: 0.9962 - val_loss: 0.5614 - val_acc: 0.9411\n",
      "Epoch 474/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0354 - acc: 0.9956 - val_loss: 0.6621 - val_acc: 0.9341\n",
      "Epoch 475/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0243 - acc: 0.9970 - val_loss: 0.6830 - val_acc: 0.9278\n",
      "Epoch 476/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0164 - acc: 0.9977 - val_loss: 0.6999 - val_acc: 0.9274\n",
      "Epoch 477/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0226 - acc: 0.9972 - val_loss: 0.5710 - val_acc: 0.9371\n",
      "Epoch 478/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0193 - acc: 0.9979 - val_loss: 0.5852 - val_acc: 0.9375\n",
      "Epoch 479/500\n",
      "110/109 [==============================] - 47s 423ms/step - loss: 0.0149 - acc: 0.9976 - val_loss: 0.6142 - val_acc: 0.9314\n",
      "Epoch 480/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0244 - acc: 0.9966 - val_loss: 0.5930 - val_acc: 0.9408\n",
      "Epoch 481/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0158 - acc: 0.9983 - val_loss: 0.5768 - val_acc: 0.9401\n",
      "Epoch 482/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0166 - acc: 0.9973 - val_loss: 0.4722 - val_acc: 0.9482\n",
      "Epoch 483/500\n",
      "110/109 [==============================] - 47s 428ms/step - loss: 0.0153 - acc: 0.9986 - val_loss: 0.4960 - val_acc: 0.9472\n",
      "Epoch 484/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0185 - acc: 0.9974 - val_loss: 0.5579 - val_acc: 0.9411\n",
      "Epoch 485/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0166 - acc: 0.9982 - val_loss: 0.6365 - val_acc: 0.9311\n",
      "Epoch 486/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0284 - acc: 0.9962 - val_loss: 0.5943 - val_acc: 0.9411\n",
      "Epoch 487/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0131 - acc: 0.9984 - val_loss: 0.5133 - val_acc: 0.9421\n",
      "Epoch 488/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0180 - acc: 0.9972 - val_loss: 0.6151 - val_acc: 0.9338\n",
      "Epoch 489/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0292 - acc: 0.9955 - val_loss: 0.4744 - val_acc: 0.9488\n",
      "Epoch 490/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0335 - acc: 0.9962 - val_loss: 0.6550 - val_acc: 0.9281\n",
      "Epoch 491/500\n",
      "110/109 [==============================] - 47s 424ms/step - loss: 0.0425 - acc: 0.9953 - val_loss: 0.5302 - val_acc: 0.9401\n",
      "Epoch 492/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0276 - acc: 0.9957 - val_loss: 0.5308 - val_acc: 0.9448\n",
      "Epoch 493/500\n",
      "110/109 [==============================] - 47s 427ms/step - loss: 0.0185 - acc: 0.9974 - val_loss: 0.5676 - val_acc: 0.9418\n",
      "Epoch 494/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0230 - acc: 0.9977 - val_loss: 0.5171 - val_acc: 0.9452\n",
      "Epoch 495/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0201 - acc: 0.9968 - val_loss: 0.5637 - val_acc: 0.9405\n",
      "Epoch 496/500\n",
      "110/109 [==============================] - 47s 423ms/step - loss: 0.0225 - acc: 0.9969 - val_loss: 0.5732 - val_acc: 0.9411\n",
      "Epoch 497/500\n",
      "110/109 [==============================] - 47s 425ms/step - loss: 0.0192 - acc: 0.9979 - val_loss: 0.4639 - val_acc: 0.9495\n",
      "Epoch 498/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0147 - acc: 0.9977 - val_loss: 0.4962 - val_acc: 0.9462\n",
      "Epoch 499/500\n",
      "110/109 [==============================] - 47s 424ms/step - loss: 0.0243 - acc: 0.9964 - val_loss: 0.5665 - val_acc: 0.9405\n",
      "Epoch 500/500\n",
      "110/109 [==============================] - 47s 426ms/step - loss: 0.0196 - acc: 0.9974 - val_loss: 0.4681 - val_acc: 0.9455\n"
     ]
    }
   ],
   "source": [
    "#epochs = 200\n",
    "# hist = model.fit( x_train, y_train_onehot, batch_size=50, shuffle=True,  epochs=epochs, \n",
    "#                  verbose=1 , callbacks=[tensorboard], validation_data=[x_val, y_val_onehot] )\n",
    "# 5, \n",
    "hist = model.fit_generator( train_generator, epochs = 500, validation_data=val_generator, \n",
    "                           callbacks=[cp_callback],\n",
    "                           steps_per_epoch=len(x_train)/batch_size, validation_steps=len(x_val)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47270005646357965, 0.9481605352765342]\n"
     ]
    }
   ],
   "source": [
    "# hist = model.evaluate( x_val/255., y_val_onehot, batch_size=30, verbose=1 )\n",
    "# or \n",
    "hist = model.evaluate_generator(val_generator, steps=len(x_val)/batch_size)\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[123 162 156 ...  43  49  93] 0 195\n"
     ]
    }
   ],
   "source": [
    "# submission\n",
    "x_test = np.load('x_test.npy')\n",
    "predictions = model.predict( x_test/255. )\n",
    "pdi = np.argmax(predictions, axis=1)\n",
    "print(pdi, np.min(pdi), np.max(pdi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_file</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_00001.jpg</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_00002.jpg</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_00003.jpg</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_00004.jpg</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_00005.jpg</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         img_file  class\n",
       "0  test_00001.jpg    124\n",
       "1  test_00002.jpg    163\n",
       "2  test_00003.jpg    157\n",
       "3  test_00004.jpg     94\n",
       "4  test_00005.jpg     18"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(basedir+'sample_submission.csv')\n",
    "submission[\"class\"] = pdi + 1  # class [0,195] to [1,196]  \n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7b498ad940>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(submission[\"class\"], order=submission[\"class\"].value_counts(ascending=True).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
